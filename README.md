# ğŸ§  AI Knowledge Hub - Intelligent Document Search Engine

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen.svg)](#)

*Transform your documents into an intelligent, searchable knowledge base with AI-powered question answering*

[ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-api-documentation) â€¢ [ğŸ¯ Features](#-features) â€¢ [ğŸ—ï¸ Architecture](#-architecture)

</div>

---

## ğŸŒŸ Overview

The **AI Knowledge Hub** is a cutting-edge RAG (Retrieval-Augmented Generation) system that transforms your document collection into an intelligent, searchable knowledge base. Upload documents in multiple formats and ask natural language questions to receive AI-synthesized answers with precise source attribution.

## âœ¨ Features

### ğŸ”¥ Core Capabilities
- ğŸ“„ **Multi-Format Support** - Upload PDF, DOCX, and TXT files seamlessly
- ğŸ” **Intelligent Search** - Advanced vector-based similarity search using state-of-the-art embeddings
- ğŸ¤– **AI-Powered Answers** - Get contextual responses powered by Groq's Llama 4 Scout model
- ğŸ“š **Source Attribution** - Every answer includes precise document references
- âš¡ **Real-time Processing** - Lightning-fast document ingestion and query processing

### ğŸ¨ Modern Interface
- ğŸŒŸ **Beautiful UI** - Stunning, responsive design with modern animations
- ğŸŒ™ **Dark/Light Mode** - Seamless theme switching for optimal viewing
- ğŸ“± **Mobile Responsive** - Perfect experience across all devices
- ğŸ¯ **Intuitive UX** - Drag-and-drop uploads and smart search interface
- ğŸ“Š **Analytics Dashboard** - Real-time statistics and performance metrics

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸŒ Modern Web Interface] --> B[âš¡ FastAPI Backend]
    B --> C[ğŸ“„ Document Processor]
    B --> D[ğŸ” RAG System]
    B --> E[ğŸ¤– LLM Client]
    
    C --> F[ğŸ“Š FAISS Vector DB]
    D --> F
    E --> G[ğŸ§  Groq API]
    
    subgraph "Document Processing"
        C --> H[Text Extraction]
        H --> I[Smart Chunking]
        I --> J[Embeddings]
    end
    
    subgraph "Query Processing"
        D --> K[Vector Search]
        K --> L[Context Assembly]
        L --> E
    end
```

### ğŸ”§ Technology Stack
- **Frontend**: Modern HTML5, CSS3, JavaScript with Tailwind CSS
- **Backend**: FastAPI (Python) with async support
- **AI/ML**: Sentence Transformers, FAISS, Groq LLM API
- **Database**: FAISS vector database with persistent storage
- **Deployment**: Uvicorn ASGI server

## ğŸ“‹ Prerequisites

- ğŸ **Python 3.8+** (Recommended: Python 3.9 or higher)
- ğŸ“¦ **pip** (Python package manager)
- ğŸ’¾ **4GB+ RAM** (For optimal performance with embeddings)
- ğŸŒ **Internet Connection** (For initial model downloads and LLM API)

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/RajkumarDake/Knowledge-base-Search-Engine.git
cd Knowledge-base-Search-Engine
```

### 2ï¸âƒ£ Install Dependencies
```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install requirements
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configuration (Optional)
```bash
# Copy environment template
cp .env.example .env

# Edit .env file with your preferences
# Note: Groq API key is pre-configured for demo purposes
```

### 4ï¸âƒ£ Launch the Application
```bash
# Option 1: Using the start script (recommended)
python start.py

# Option 2: Direct FastAPI launch
python main.py

# Option 3: Production mode
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 5ï¸âƒ£ Access the Interface
- ğŸŒ **Web Interface**: [http://localhost:8000](http://localhost:8000)
- ğŸ“š **API Documentation**: [http://localhost:8000/docs](http://localhost:8000/docs)
- ğŸ”§ **Health Check**: [http://localhost:8000/health](http://localhost:8000/health)

### 6ï¸âƒ£ Start Using
1. **ğŸ“¤ Upload Documents** - Drag & drop or browse files (PDF, DOCX, TXT)
2. **â“ Ask Questions** - Type natural language queries
3. **ğŸ¯ Get Answers** - Receive AI-powered responses with source references
4. **ğŸ“Š Monitor Stats** - View analytics and performance metrics

## API Documentation

### Endpoints

#### `POST /upload`
Upload and process documents
- **Body**: Multipart form data with file
- **Response**: Processing status and document metadata

#### `POST /query`
Query the knowledge base
- **Body**: `{"query": "your question", "max_chunks": 3}`
- **Response**: AI-generated answer with sources

#### `GET /search`
Search for relevant document chunks
- **Query**: `?query=search_term&top_k=5`
- **Response**: Ranked list of relevant chunks

#### `GET /stats`
Get knowledge base statistics
- **Response**: Document and chunk counts

#### `GET /health`
Health check endpoint
- **Response**: Service status

## âš™ï¸ Configuration

### ğŸ”§ Environment Variables

| Variable | Default Value | Description | Required |
|----------|---------------|-------------|----------|
| `GROQ_API_KEY` | *Pre-configured* | Groq API key for LLM access | âœ… |
| `GROQ_MODEL` | `meta-llama/llama-4-scout-17b-16e-instruct` | LLM model identifier | âŒ |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Sentence transformer model | âŒ |
| `MAX_CHUNK_SIZE` | `1000` | Maximum characters per text chunk | âŒ |
| `CHUNK_OVERLAP` | `200` | Character overlap between chunks | âŒ |
| `VECTOR_DB_PATH` | `./vector_db` | Path for FAISS index storage | âŒ |
| `UPLOAD_DIR` | `./uploads` | Directory for uploaded files | âŒ |

### ğŸ›ï¸ Advanced Configuration
```python
# config.py - Customize these settings
CONFIG = {
    "llm_temperature": 0.3,        # Creativity vs accuracy balance
    "max_tokens": 1000,            # Maximum response length
    "top_k_chunks": 3,             # Number of relevant chunks to retrieve
    "similarity_threshold": 0.7,    # Minimum similarity for chunk inclusion
}
```

### ğŸ“ Project Structure

```
ğŸ“¦ Knowledge-base-Search-Engine/
â”œâ”€â”€ ğŸš€ main.py                    # FastAPI application entry point
â”œâ”€â”€ âš™ï¸ config.py                 # Configuration and settings
â”œâ”€â”€ ğŸ“„ document_processor.py     # Multi-format document processing
â”œâ”€â”€ ğŸ§  rag_system.py            # RAG implementation with FAISS
â”œâ”€â”€ ğŸ¤– llm_client.py            # Groq LLM API integration
â”œâ”€â”€ ğŸ¬ start.py                 # Application launcher
â”œâ”€â”€ ğŸ“‹ requirements.txt         # Python dependencies
â”œâ”€â”€ ğŸ”’ .env.example            # Environment variables template
â”œâ”€â”€ ğŸ“š README.md               # This documentation
â”œâ”€â”€ ğŸ“– PROJECT_OVERVIEW.md     # Detailed project overview
â”œâ”€â”€ ğŸŒ frontend/               # Modern web interface
â”‚   â”œâ”€â”€ ğŸ  index.html         # Main application page
â”‚   â””â”€â”€ ğŸ“ static/
â”‚       â””â”€â”€ âš¡ app.js          # Enhanced JavaScript functionality
â”œâ”€â”€ ğŸ“¤ uploads/                # Document storage (auto-created)
â”œâ”€â”€ ğŸ—„ï¸ vector_db/             # FAISS vector database (auto-created)
â””â”€â”€ ğŸ __pycache__/           # Python cache (auto-generated)
```

## How It Works

### 1. Document Processing
- **Text Extraction**: Extracts text from PDF, DOCX, and TXT files
- **Chunking**: Splits documents into overlapping chunks for better retrieval
- **Cleaning**: Normalizes text and removes extra whitespace

### 2. Vector Embeddings
- **Encoding**: Uses sentence-transformers to create embeddings
- **Storage**: Stores vectors in FAISS index for fast similarity search
- **Normalization**: Normalizes embeddings for cosine similarity

### 3. Retrieval Process
- **Query Encoding**: Converts user query to vector embedding
- **Similarity Search**: Finds most relevant document chunks
- **Context Assembly**: Combines relevant chunks for LLM input

### 4. Answer Generation
- **Prompt Engineering**: Creates structured prompts for the LLM
- **API Integration**: Calls Groq API with context and query
- **Response Processing**: Formats and returns synthesized answer

## ğŸ’¡ Usage Examples

### ğŸ” Basic Information Retrieval
```
â“ Query: "What is machine learning?"
ğŸ¤– Response: "Machine learning is a subset of artificial intelligence that enables 
            computers to learn and improve from experience without being explicitly 
            programmed. According to the uploaded documents..."
ğŸ“š Sources: [document1.pdf, pages 15-17]
```

### ğŸ“Š Complex Analysis
```
â“ Query: "Compare the advantages and disadvantages mentioned in the documents"
ğŸ¤– Response: "Based on the analysis of your documents, here's a comprehensive 
            comparison: Advantages include... Disadvantages involve..."
ğŸ“š Sources: [report.docx, section 3.2], [analysis.pdf, chapter 5]
```

### ğŸ¯ Specific Research
```
â“ Query: "What are the key findings from the research paper?"
ğŸ¤– Response: "The research paper identifies three key findings: 1) Performance 
            improved by 23%... 2) Cost reduction of 15%... 3) User satisfaction..."
ğŸ“š Sources: [research_paper.pdf, abstract & conclusion]
```

### ğŸ“ˆ Data Insights
```
â“ Query: "What trends are mentioned in the quarterly reports?"
ğŸ¤– Response: "The quarterly reports highlight several important trends: Revenue 
            growth of 12%, market expansion in Asia, and increased digital adoption..."
ğŸ“š Sources: [Q1_report.pdf], [Q2_report.pdf], [market_analysis.docx]
```

## Technical Details

### RAG Implementation
- **Embedding Model**: `all-MiniLM-L6-v2` (384-dimensional vectors)
- **Vector Database**: FAISS with inner product similarity
- **Chunking Strategy**: Sentence-aware with configurable overlap
- **Retrieval**: Top-k similarity search with score thresholding

### LLM Integration
- **Provider**: Groq Cloud API
- **Model**: Llama 4 Scout 17B Instruct
- **Temperature**: 0.3 (balanced creativity/accuracy)
- **Max Tokens**: 1000 per response

### Performance Optimizations
- **Batch Processing**: Efficient document processing
- **Persistent Storage**: FAISS index saved to disk
- **Async Operations**: Non-blocking API endpoints
- **Memory Management**: Optimized embedding storage

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues & Solutions

<details>
<summary><strong>ğŸ Python/Package Issues</strong></summary>

**Import Errors**
```bash
# Upgrade all packages
pip install --upgrade -r requirements.txt

# Force reinstall if needed
pip install --force-reinstall -r requirements.txt
```

**FAISS Installation Problems**
```bash
# For CPU-only version
pip install faiss-cpu --no-cache-dir

# For GPU version (if CUDA available)
pip install faiss-gpu --no-cache-dir
```
</details>

<details>
<summary><strong>ğŸŒ API & Connection Issues</strong></summary>

**LLM API Problems**
- âœ… Check internet connection
- âœ… Verify API key in `config.py`
- âœ… Test connection: `GET /test-llm`
- âœ… Check API rate limits

**Server Won't Start**
```bash
# Check if port is in use
netstat -an | grep :8000

# Use different port
uvicorn main:app --port 8001
```
</details>

<details>
<summary><strong>ğŸ“„ File Upload Issues</strong></summary>

**Supported Formats**: PDF, DOCX, TXT only
**File Size Limit**: 50MB per file
**Common Fixes**:
- Ensure `uploads/` directory exists and is writable
- Check file permissions
- Verify file isn't corrupted
- Try smaller files first
</details>

### ğŸš€ Performance Optimization

| Issue | Solution | Impact |
|-------|----------|--------|
| ğŸŒ Slow Processing | Break large documents into smaller chunks | âš¡ 2-3x faster |
| ğŸ§  High Memory Usage | Reduce `MAX_CHUNK_SIZE` in config | ğŸ’¾ 50% less RAM |
| â“ Poor Query Results | Use specific, well-formed questions | ğŸ¯ Better accuracy |
| ğŸ” Slow Search | Reduce `top_k_chunks` parameter | âš¡ Faster responses |

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### ğŸ”„ Development Workflow
1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **ğŸ’» Make** your changes
4. **âœ… Test** thoroughly
5. **ğŸ“ Commit** your changes (`git commit -m 'Add amazing feature'`)
6. **ğŸš€ Push** to the branch (`git push origin feature/amazing-feature`)
7. **ğŸ“¬ Submit** a Pull Request

### ğŸ¯ Areas for Contribution
- ğŸ› **Bug Fixes** - Help us squash bugs
- âœ¨ **New Features** - Add exciting functionality
- ğŸ“š **Documentation** - Improve guides and examples
- ğŸ¨ **UI/UX** - Enhance the user interface
- âš¡ **Performance** - Optimize speed and efficiency
- ğŸ§ª **Testing** - Add comprehensive test coverage

### ğŸ“‹ Development Setup
```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/Knowledge-base-Search-Engine.git

# Install development dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt  # If available

# Run tests
python -m pytest tests/  # If test suite exists
```

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

Special thanks to the amazing open-source community and these fantastic projects:

- ğŸ¤– **[Groq](https://groq.com/)** - Lightning-fast LLM inference
- ğŸ” **[Sentence Transformers](https://www.sbert.net/)** - State-of-the-art embeddings
- âš¡ **[FAISS](https://github.com/facebookresearch/faiss)** - Efficient similarity search
- ğŸš€ **[FastAPI](https://fastapi.tiangolo.com/)** - Modern Python web framework
- ğŸ¨ **[Tailwind CSS](https://tailwindcss.com/)** - Utility-first CSS framework
- ğŸ¦„ **[Uvicorn](https://www.uvicorn.org/)** - Lightning-fast ASGI server

## ğŸ“Š Project Stats

![GitHub stars](https://img.shields.io/github/stars/RajkumarDake/Knowledge-base-Search-Engine?style=social)
![GitHub forks](https://img.shields.io/github/forks/RajkumarDake/Knowledge-base-Search-Engine?style=social)
![GitHub issues](https://img.shields.io/github/issues/RajkumarDake/Knowledge-base-Search-Engine)
![GitHub pull requests](https://img.shields.io/github/issues-pr/RajkumarDake/Knowledge-base-Search-Engine)

---

<div align="center">

**ğŸ§  Built with â¤ï¸ for intelligent document search and AI-powered question answering**

*Transform your documents into knowledge. Transform your knowledge into insights.*

[â­ Star this repo](https://github.com/RajkumarDake/Knowledge-base-Search-Engine) â€¢ [ğŸ› Report Bug](https://github.com/RajkumarDake/Knowledge-base-Search-Engine/issues) â€¢ [ğŸ’¡ Request Feature](https://github.com/RajkumarDake/Knowledge-base-Search-Engine/issues)

</div>
